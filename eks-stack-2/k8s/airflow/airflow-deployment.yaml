---
# MySQL deployment for Airflow backend
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-mysql
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow-mysql
  template:
    metadata:
      labels:
        app: airflow-mysql
    spec:
      containers:
      - name: mysql
        image: mysql:5.7
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        ports:
        - containerPort: 3306
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: "airflow"
        - name: MYSQL_DATABASE
          value: "airflow"
        - name: MYSQL_USER
          value: "airflow"
        - name: MYSQL_PASSWORD
          value: "airflow"
        args:
        - --explicit_defaults_for_timestamp=1
        - --sql-mode=NO_ENGINE_SUBSTITUTION
---
# MySQL Service
apiVersion: v1
kind: Service
metadata:
  name: airflow-mysql
spec:
  ports:
  - port: 3306
  selector:
    app: airflow-mysql
---
# Airflow Webserver Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-webserver
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow-webserver
  template:
    metadata:
      labels:
        app: airflow-webserver
    spec:
      initContainers:
      - name: init-db
        image: apache/airflow:2.7.1
        env:
        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          value: "mysql://airflow:airflow@airflow-mysql:3306/airflow"
        command:
        - "/bin/bash"
        - "-c"
        - "airflow db init && airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.com"
      - name: init-dags
        image: busybox
        command: ["/bin/sh", "-c"]
        args:
        - |
          cat > /airflow-dags/dag_1.py << 'EOF'
          from airflow import DAG
          from airflow.operators.python_operator import PythonOperator
          from datetime import datetime, timedelta

          default_args = {
              'owner': 'airflow',
              'depends_on_past': False,
              'start_date': datetime(2024, 1, 1),
              'email_on_failure': False,
              'email_on_retry': False,
              'retries': 1,
              'retry_delay': timedelta(minutes=5),
          }

          def hello_world():
              print("Hello World from Airflow!")

          dag = DAG(
              'hello_world_dag',
              default_args=default_args,
              description='A simple Hello World DAG',
              schedule_interval=timedelta(days=1),
          )

          hello_task = PythonOperator(
              task_id='hello_task',
              python_callable=hello_world,
              dag=dag,
          )
          EOF
        volumeMounts:
        - name: airflow-dags
          mountPath: /airflow-dags
      containers:
      - name: webserver
        image: apache/airflow:2.7.1
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        ports:
        - containerPort: 8080
        env:
        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          value: "mysql://airflow:airflow@airflow-mysql:3306/airflow"
        - name: AIRFLOW__CORE__EXECUTOR
          value: "LocalExecutor"
        - name: AIRFLOW__CORE__LOAD_EXAMPLES
          value: "false"
        - name: AIRFLOW__WEBSERVER__WORKERS
          value: "4"
        - name: AIRFLOW__WEBSERVER__WORKER_REFRESH_INTERVAL
          value: "3600"
        command:
        - "airflow"
        args:
        - "webserver"
        volumeMounts:
        - name: airflow-dags
          mountPath: /opt/airflow/dags
      volumes:
      - name: airflow-dags
        emptyDir: {}
---
# Airflow Webserver Service
apiVersion: v1
kind: Service
metadata:
  name: airflow-webserver
spec:
  type: LoadBalancer
  ports:
  - port: 8080
  selector:
    app: airflow-webserver
---
# Airflow Scheduler Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-scheduler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow-scheduler
  template:
    metadata:
      labels:
        app: airflow-scheduler
    spec:
      initContainers:
      - name: init-dags
        image: busybox
        command: ["/bin/sh", "-c"]
        args:
        - |
          cat > /airflow-dags/dag_1.py << 'EOF'
          from airflow import DAG
          from airflow.operators.python_operator import PythonOperator
          from datetime import datetime, timedelta

          default_args = {
              'owner': 'airflow',
              'depends_on_past': False,
              'start_date': datetime(2024, 1, 1),
              'email_on_failure': False,
              'email_on_retry': False,
              'retries': 1,
              'retry_delay': timedelta(minutes=5),
          }

          def hello_world():
              print("Hello World from Airflow!")

          dag = DAG(
              'hello_world_dag',
              default_args=default_args,
              description='A simple Hello World DAG',
              schedule_interval=timedelta(days=1),
          )

          hello_task = PythonOperator(
              task_id='hello_task',
              python_callable=hello_world,
              dag=dag,
          )
          EOF
        volumeMounts:
        - name: airflow-dags
          mountPath: /airflow-dags
      containers:
      - name: scheduler
        image: apache/airflow:2.7.1
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        env:
        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          value: "mysql://airflow:airflow@airflow-mysql:3306/airflow"
        - name: AIRFLOW__CORE__EXECUTOR
          value: "LocalExecutor"
        - name: AIRFLOW__CORE__LOAD_EXAMPLES
          value: "false"
        - name: AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC
          value: "10"
        - name: AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL
          value: "30"
        command:
        - "airflow"
        args:
        - "scheduler"
        volumeMounts:
        - name: airflow-dags
          mountPath: /opt/airflow/dags
      volumes:
      - name: airflow-dags
        emptyDir: {}